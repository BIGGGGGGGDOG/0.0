{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BIGGGGGGGDOG/0.0/blob/main/EIE546_Image_Video_Processing_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1: Introduction to EIE546 Video Technology in Python\n",
        "Lab1 Session  \n",
        "Speaker: Ms. ZHANG Yi   \n",
        "Contributor: Dr. WANG Yi, Mr. YAO Lei, Ms. ZHANG Yi\n",
        ">This Lab provides an introduction to basic image and video processing techniques using the OpenCV computer vision library and some standard data analysis libraries in Python. Knowledge of Python programming is not required but will help.  \n",
        "\n",
        "*The source of this notebook is located at [here](https://github.com/RayYoh/EIE546_Image-Video-Processing/blob/main/EIE546_Image_Video_Processing_Lab1.ipynb)*\n",
        "\n",
        "*Useful intro about [Colab](https://colab.research.google.com/notebooks/welcome.ipynb)*\n",
        "\n",
        "*Useful intro about [OpenCV](https://opencv.org/)*"
      ],
      "metadata": {
        "id": "-1F1LqmSNBUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section A. Reading and Displaying Image"
      ],
      "metadata": {
        "id": "3WlGxVVZhUcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install Some Necessary Dependencies\n",
        "> This step loads some required libraries used in this notebook: **numpy**, **opencv-python**, **yaml**, **matplotlib**, **scipy**, **skimage**.\n",
        "\n",
        "* [Numpy](https://www.numpy.org/) is an array manipulation library, used for linear algebra, Fourier transform, and random number capabilities.\n",
        "* [Opencv-python](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_image_display/py_image_display.html) is a library for computer vision tasks.\n",
        "* [Pyyaml](https://pyyaml.org/) is a full-featured YAML framework for the Python programming language.\n",
        "* [Matplotlib](https://matplotlib.org/) is a library which generates figures and provides graphical user interface toolkit.\n",
        "* [Skimage](https://scikit-image.org/) scikit-image is a collection of algorithms for image processing."
      ],
      "metadata": {
        "id": "X664H9A1VbKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n",
        "!pip install pyyaml\n",
        "!pip install numpy\n",
        "!pip install scikit-image\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "fDbzwbleVtX0",
        "outputId": "5f3cf530-e395-40bb-fd3f-52fe9fa69e2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (1.16.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJgGk5RRvxhD"
      },
      "source": [
        "### Step 2: Load Some Necessary Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLGuhlvPdf90"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import yaml\n",
        "import numpy as np\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsG4iyW0v7eu"
      },
      "source": [
        "### Step 3: Read an Image Config File.\n",
        ">In this step we will read an image config file from `image.yaml`.  \n",
        "\n",
        "* The `open(filename, mode)` function is used to open a file in Python and return a file object. It takes two parameters - filename and mode. Mode refers to the mode in which the file has to be opened, such as `'r'` for read mode, `'w'` for write mode, `'a'` for append mode etc. It returns a file object using which you can read, write or append to the file.\n",
        "* The `yaml.safe_load(f)` function in Python is used to parse a YAML document from a file object safely. It takes a file object `f` as parameter, opened in read mode and parses the YAML contents from that file object.It returns the parsed YAML data as a Python dictionary or list.\n",
        "1. Open the YAML config file `image.yaml` in read mode.\n",
        "2. Use `yaml.safe_load(f)` to parse the YAML contents into a Python dictionary cfg.\n",
        "3. Extract the raw file path, `width` and `height` from the cfg dictionary.\n",
        "4. Now `raw_file` contains the path to the image file, width and height are extracted as integers to be used later for reshaping the array.\n",
        "5. The image can then be read and processed using the `raw_file`, `width` and `height` variables initialized from the YAML config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXjk6rRgeWlq"
      },
      "outputs": [],
      "source": [
        "with open('image.yaml', 'r') as f:\n",
        "  cfg = yaml.safe_load(f)\n",
        "\n",
        "raw_file, width, height = cfg['InputFile'], cfg['Width'], cfg['Height']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write the code to print the variables (InputFile, Width, and Height).**"
      ],
      "metadata": {
        "id": "mvyJZaVVycIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print()\n",
        "#\n",
        "#"
      ],
      "metadata": {
        "id": "7nkZnN4IyUER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Read Image and Display\n",
        "> In this step we will read an image from `FourPeople_1280x720.raw`, and display it.  \n",
        "\n",
        "`*.raw` reads an image in YUV format, which needs to be converted to the BGR domain for display. Please note the difference in RGB and BGR format. The default input color channels are in BGR format for OpenCV.\n",
        "1. `np.fromfile` reads the raw YUV data from the file into a NumPy array, with 8-bit unsigned integers (0-255 range).\n",
        "2. `yuv_image.reshape` reshapes the flat array into a 2D image with appropriate height and width based on YUV 4:2:0 format.\n",
        "3. `cv2.cvtColor` converts the YUV image to BGR color space.\n",
        "4. Another `cvtColor` converts BGR to RGB for better display.\n",
        "5. `matplotlib` is used to display the BGR and RGB image side-by-side."
      ],
      "metadata": {
        "id": "OE7FXxEUQCjO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXQqOU0KehkT"
      },
      "outputs": [],
      "source": [
        "yuv_image = np.fromfile(raw_file, dtype=np.uint8)\n",
        "yuv_image = yuv_image.reshape((height * 3 // 2, width))\n",
        "\n",
        "bgr_image = cv2.cvtColor(yuv_image, cv2.COLOR_YUV2BGR_I420)\n",
        "rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
        "f, (plt1, plt2) = plt.subplots(1, 2, figsize=(30, 30))\n",
        "plt1.set_title('BGR');plt1.imshow(bgr_image)\n",
        "plt2.axis('off'); plt2.set_title('RGB'); plt2.imshow(rgb_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section B. Image Processing and Storing Image"
      ],
      "metadata": {
        "id": "CSKP2UApmNII"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYkjWrQ9g-GP"
      },
      "source": [
        "### Step 5: Display GrayScale Image\n",
        "> In this step we will modify YUV to make the image grayscale in the BGR domain.  \n",
        "\n",
        "This code converts a YUV image to grayscale by:\n",
        "1. Making a copy of the Y channel from the original YUV image into `yuv_gray`.\n",
        "2. Creating an empty 3 channel RGB image called `gray_image`.\n",
        "3. Copying the `yuv_gray` channel into each of the R, G and B channels of `gray_image`.\n",
        "4. Writing out `gray_image` to a file.\n",
        "5. Displaying `gray_image`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrXkFXdBhBa2"
      },
      "outputs": [],
      "source": [
        "yuv_gray = yuv_image.copy()\n",
        "gray_image = np.zeros((height, width, 3),dtype=np.uint8)\n",
        "for i in range(3):\n",
        "  gray_image[:,:,i] = yuv_gray[:height][:]\n",
        "cv2.imwrite('grayscale.png', gray_image)\n",
        "plt.imshow(gray_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b64BEw5vWMd"
      },
      "source": [
        "### Step 6: Display Flipped Image\n",
        "> In this step we will modify YUV to make the image flipped in the BGR domain.  \n",
        "\n",
        "* `np.flip()` function in NumPy is used to flip or reverse the order of elements in an array along a specified axis. It takes an array and an integer axis as input. The axis indicates the dimension along which flipping is to be done. It reverses the order of elements along the given axis. The original array is not changed, instead a new flipped array is returned. Axes 0 and 1 represent vertical and horizontal flipping respectively for 2D arrays.  \n",
        "* `np.vstack()` is a function in NumPy that is used to vertically stack arrays. It concatenates arrays along the first axis (axis 0). It requires input arrays to have same number of columns. Stacking is done along axis 0 by default. It returns a new array containing the stacked rows and original arrays remain unchanged.\n",
        "1. Take a copy of the original YUV image array.\n",
        "2. Extract Y, U and V channels by slicing based on standard YUV 4:2:0 dimensions.\n",
        "3. Flip each channel array vertically using `np.flip(axis=0)`.\n",
        "4. Stack the flipped Y, U and V channels back together into a full YUV array.\n",
        "5. Convert flipped YUV to BGR for visualization.\n",
        "6. Convert BGR to RGB for correct color display.\n",
        "7. Display the resulting flipped image with `imshow()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv85ToicvYKu"
      },
      "outputs": [],
      "source": [
        "yuv_flipped = yuv_image.copy()\n",
        "y_flipped = np.flip(yuv_flipped[:height][:], axis=0)\n",
        "u_flipped = np.flip(yuv_flipped[height:int(height * 1.25)][:], axis=0)\n",
        "v_flipped = np.flip(yuv_flipped[int(height * 1.25):][:], axis=0)\n",
        "yuv_flipped = np.vstack([y_flipped, u_flipped, v_flipped])\n",
        "flip_bgr_image = cv2.cvtColor(yuv_flipped, cv2.COLOR_YUV2BGRA_I420)\n",
        "flip_rgb_image = cv2.cvtColor(flip_bgr_image, cv2.COLOR_BGR2RGB)\n",
        "cv2.imwrite('flip.png', flip_bgr_image)\n",
        "plt.imshow(flip_rgb_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write the code to flip the image left and right, and show the flipped results.**\n"
      ],
      "metadata": {
        "id": "uap-oraazfIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# yuv_lr_flipped = yuv_image.copy()\n",
        "#\n",
        "#"
      ],
      "metadata": {
        "id": "s90HoAhBzhtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image as a 3D Array"
      ],
      "metadata": {
        "id": "jfqruKdsfPQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Image as a 3D Array\n",
        "> In this step we will visualize and compare the RGB channels of an image using matplotlib.  \n",
        "\n",
        "1. Read lena image as unchanged with alpha channel using cv2.\n",
        "2. Convert it from BGR to RGB for correct color representation.\n",
        "3. Create a figure with 1 row and 4 columns of plots using `subplots`.\n",
        "4. Set a large figsize for better visibility.\n",
        "5. Plot the original RGB image in the first subplot.\n",
        "6. Slice just the R channel and plot as grayscale in subplot 2.\n",
        "7. Similarly, visualize G and B channels in subsequent plots.\n",
        "8. Turn off axes and set titles for each subplot."
      ],
      "metadata": {
        "id": "rkq9m9HLHc3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lena = cv2.imread('lena.png', cv2.IMREAD_UNCHANGED)\n",
        "lena_rgb = cv2.cvtColor(lena, cv2.COLOR_BGR2RGB)\n",
        "f, (plt1, plt2, plt3, plt4) = plt.subplots(1, 4, figsize=(30, 30))\n",
        "\n",
        "plt1.set_title('Original'); plt1.imshow(lena_rgb);\n",
        "plt2.axis('off'); plt2.set_title('Red'); plt2.imshow(lena_rgb[:,:,0], cmap='Reds');\n",
        "plt3.axis('off'); plt3.set_title('Green'); plt3.imshow(lena_rgb[:,:,1], cmap='Greens');\n",
        "plt4.axis('off'); plt4.set_title('Blue'); plt4.imshow(lena_rgb[:,:,2], cmap='Blues');"
      ],
      "metadata": {
        "id": "FujGafhvH71D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write code to show the YUV channels of YUV4:4:4 and show the results.**"
      ],
      "metadata": {
        "id": "qaQvV_3kz57J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#\n",
        "#"
      ],
      "metadata": {
        "id": "cYdE6OyWz6-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8. Display zoomed-in regions\n",
        "> We will extract and visualize zoomed-in regions from the Lena image using array slicing and matplotlib.  \n",
        "\n",
        "1. Create a figure with 4 subplots again to arrange images.\n",
        "2. Extract head region coordinates from full image.\n",
        "3. Slice head from the Lena RGB array using those coordinates.\n",
        "4. Similarly, extract eye region from head and zoom further for `zoomed_eye`.\n",
        "5. Plot original Lena image in first subplot. Visualize head, eye and zoomed eye regions in subsequent plots.\n",
        "6. Turn off axes and use nearest neighbor interpolation for pixelated look."
      ],
      "metadata": {
        "id": "KToDvPCHdhcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, (plt1, plt2, plt3, plt4) = plt.subplots(1, 4, figsize=(30, 30))\n",
        "\n",
        "head = lena_rgb[200:350, 200:350]\n",
        "eye = head[40:80, 40:80]\n",
        "zoomed_eye = eye[20:30, 25:35]\n",
        "\n",
        "plt1.axis('off'); plt1.imshow(lena_rgb, interpolation='nearest');\n",
        "plt2.axis('off'); plt2.imshow(head, interpolation='nearest');\n",
        "plt3.axis('off'); plt3.imshow(eye, interpolation='nearest');\n",
        "plt4.axis('off'); plt4.imshow(zoomed_eye, interpolation='nearest');"
      ],
      "metadata": {
        "id": "zYPh9FsLKSQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print pixel values of the zoomed_eye in R, G, and B channels, respectively. (using “numpy.transpose()” function)**"
      ],
      "metadata": {
        "id": "c_MklYiLV3si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#\n",
        "#"
      ],
      "metadata": {
        "id": "Xj-XCH1HV9Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Filters\n",
        "1. Apply convolution on Lena's grayscale image using 3 different kernels - edge detect, sharpen and blur.\n",
        "2. Create 4 subplots to visualize the results.\n",
        "3. Plot original Lena grayscale in first subplot. Show convolution outputs in next 3 plots - detecting edges, sharpening and blurring the image.\n",
        "4. Use grayscale colormap and nearest neighbor interpolation.\n",
        "5. Turn off axes and add titles."
      ],
      "metadata": {
        "id": "CWuVcbTqXMdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_edge_detect3 = np.array([[-1.,-1.,-1.],\n",
        "                [-1.,8.,-1.],\n",
        "                [-1.,-1.,-1.]])\n",
        "\n",
        "kernel_sharpen2 = np.array([[-1.,-1.,-1.],\n",
        "              [-1.,9.,-1.],\n",
        "              [-1.,-1.,-1.]])\n",
        "\n",
        "kernel_blur = 1/ 9 * np.array([[1.,1.,1.],\n",
        "            [1.,1.,1.],\n",
        "            [1.,1.,1.]])"
      ],
      "metadata": {
        "id": "KGF7F6SLOXFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lena_gray = cv2.cvtColor(lena, cv2.COLOR_BGR2GRAY)\n",
        "img_edge_detect = cv2.filter2D(lena_gray, -1, kernel_edge_detect3)\n",
        "img_sharpen = cv2.filter2D(lena_gray, -1, kernel_sharpen2)\n",
        "img_blur = cv2.filter2D(lena_gray, -1, kernel_blur)\n",
        "\n",
        "f, (plt1, plt2, plt3, plt4) = plt.subplots(1, 4, figsize=(30,30))\n",
        "\n",
        "plt1.set_title('Original');plt1.imshow(lena_gray, cmap='gray', interpolation='nearest');\n",
        "# showing each channel img[x,y,color_plane]\n",
        "plt2.axis('off');plt2.set_title('Edge detect');plt2.imshow(img_edge_detect,cmap='gray', interpolation='nearest');\n",
        "plt3.axis('off');plt3.set_title('Sharpen');plt3.imshow(img_sharpen,cmap='gray', interpolation='nearest');\n",
        "plt4.axis('off');plt4.set_title('Blur');plt4.imshow(img_blur,cmap='gray', interpolation='nearest');"
      ],
      "metadata": {
        "id": "wfgMdI5EOlwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PSNR\n",
        "Here are some key mathematical equations behind PSNR calculation:  \n",
        "Mean Squared Error (MSE):\n",
        "$$MSE = \\frac{1}{mn} \\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} [I(i,j) - K(i,j)]^2$$\n",
        "Where:\n",
        "- `m, n` are image dimensions\n",
        "- `I` is the original image\n",
        "- `K` is the processed image  \n",
        "\n",
        "This calculates the average squared difference between corresponding pixels.  \n",
        "\n",
        "Peak Signal-to-Noise Ratio (PSNR):\n",
        "$$PSNR = 10 \\log_{10}\\left(\\frac{MAX_I^2}{MSE}\\right)$$\n",
        "Where:\n",
        "- `MAX_I` is maximum possible pixel value (255 for 8-bit)\n",
        "- `MSE` is mean squared error  \n",
        "\n",
        "Also can use:\n",
        "$$PSNR = 20 \\log_{10}\\left(\\frac{255}{\\sqrt{MSE}}\\right)$$\n",
        "So PSNR represents the ratio between maximum signal power and noise power in decibels.\n",
        "These quantify the quality of the processed image compared to the original. Higher PSNR indicates better quality with lower distortion."
      ],
      "metadata": {
        "id": "mSdfPIcSjbu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate PSNR between the original grayscale image (“lena_gray”) and the blurred image (“img_blur”) and give the corresponding code.**"
      ],
      "metadata": {
        "id": "Ev8ML6JR0WQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#\n",
        "#"
      ],
      "metadata": {
        "id": "meqLILLs0eL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}